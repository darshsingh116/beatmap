{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataReadFromDisk import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame loaded from: E:\\Notebooks\\osu\\processed-metadata\\processed-metadata.csv\n"
     ]
    }
   ],
   "source": [
    "df = load_df_from_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['total_length'] < 600]\n",
    "# df.iloc[7]\n",
    "# df = df[df[\"audio\"] == \"0013ddfd8bd55fdccc0b253e313b7a60\"]\n",
    "df = df[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>audio</th>\n",
       "      <th>split</th>\n",
       "      <th>folder</th>\n",
       "      <th>beatmapset_id</th>\n",
       "      <th>beatmap_id</th>\n",
       "      <th>approved</th>\n",
       "      <th>total_length</th>\n",
       "      <th>hit_length</th>\n",
       "      <th>version</th>\n",
       "      <th>file_md5</th>\n",
       "      <th>...</th>\n",
       "      <th>difficultyrating</th>\n",
       "      <th>StackLeniency</th>\n",
       "      <th>DistanceSpacing</th>\n",
       "      <th>BeatDivisor</th>\n",
       "      <th>HPDrainRate</th>\n",
       "      <th>CircleSize</th>\n",
       "      <th>OverallDifficulty</th>\n",
       "      <th>ApproachRate</th>\n",
       "      <th>SliderTickRate</th>\n",
       "      <th>SliderMultiplier</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00010f0eb02ee131aacac54bf72d5444</td>\n",
       "      <td>train</td>\n",
       "      <td>5bea04ca275826768fa4f8411822c388</td>\n",
       "      <td>830266</td>\n",
       "      <td>1739583</td>\n",
       "      <td>1</td>\n",
       "      <td>221</td>\n",
       "      <td>215</td>\n",
       "      <td>Zero Vector</td>\n",
       "      <td>00010f0eb02ee131aacac54bf72d5444</td>\n",
       "      <td>...</td>\n",
       "      <td>4.93655</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 54 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              audio  split                            folder  \\\n",
       "0  00010f0eb02ee131aacac54bf72d5444  train  5bea04ca275826768fa4f8411822c388   \n",
       "\n",
       "   beatmapset_id  beatmap_id  approved  total_length  hit_length      version  \\\n",
       "0         830266     1739583         1           221         215  Zero Vector   \n",
       "\n",
       "                           file_md5  ...  difficultyrating  StackLeniency  \\\n",
       "0  00010f0eb02ee131aacac54bf72d5444  ...           4.93655            0.0   \n",
       "\n",
       "   DistanceSpacing  BeatDivisor  HPDrainRate  CircleSize  OverallDifficulty  \\\n",
       "0              1.2            4          5.0         5.0                8.0   \n",
       "\n",
       "   ApproachRate SliderTickRate SliderMultiplier  \n",
       "0           8.0              1              1.2  \n",
       "\n",
       "[1 rows x 54 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testa,testb = process_audio_and_beatmap_for_model_single(df.iloc[0])\n",
    "# testa.shape\n",
    "# # for h in testb:\n",
    "# #     print(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df.iterrows():\n",
    "    audio,beatmap,params = new_process_audio_and_beatmap_for_model_single(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(939, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beatmap.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3194.    0.    5.]\n",
      "[3408.    0.    5.]\n",
      "[3622.    0.    1.]\n",
      "[3837.    0.    5.]\n",
      "[4051.    0.    1.]\n",
      "[4265.    0.    5.]\n",
      "[5122.    0.    1.]\n",
      "[5979.    0.    5.]\n",
      "[6622.    0.    1.]\n",
      "[7265.    0.    5.]\n",
      "[7694.    0.    1.]\n",
      "[8337.    0.    1.]\n",
      "[10051.     0.     5.]\n",
      "[10265.     0.     5.]\n",
      "[10479.     0.     1.]\n",
      "[10694.     0.     5.]\n",
      "[10908.     0.     1.]\n",
      "[11122.     0.     5.]\n",
      "[11979.     0.     1.]\n",
      "[12836.     0.     5.]\n",
      "[13479.     0.     1.]\n",
      "[14122.     0.     5.]\n",
      "[14551.     0.     1.]\n",
      "[16265.     0.     1.]\n",
      "[21408.     0.     5.]\n",
      "[21515.     0.     1.]\n",
      "[21622.     0.     1.]\n",
      "[21837.     0.     5.]\n",
      "[21944.     0.     1.]\n",
      "[22051.     0.     1.]\n",
      "[22265.     0.     5.]\n",
      "[22372.     0.     1.]\n",
      "[22479.     0.     1.]\n",
      "[22694.     0.     5.]\n",
      "[22801.     0.     1.]\n",
      "[22908.     0.     1.]\n",
      "[23122.     0.     5.]\n",
      "[23229.     0.     1.]\n",
      "[23337.     0.     1.]\n",
      "[23551.     0.     5.]\n",
      "[23658.     0.     1.]\n",
      "[23765.     0.     1.]\n",
      "[23979.     0.     5.]\n",
      "[24194.     0.     1.]\n",
      "[24622.     0.     1.]\n",
      "[24837.     0.     5.]\n",
      "[25479.     0.     1.]\n",
      "[26551.     0.     5.]\n",
      "[27193.     0.     1.]\n",
      "[28265.     0.     5.]\n",
      "[28479.     0.     1.]\n",
      "[28694.     0.     5.]\n",
      "[28908.     0.     1.]\n",
      "[29122.     0.     5.]\n",
      "[29336.     0.     1.]\n",
      "[29551.     0.     5.]\n",
      "[29765.     0.     1.]\n",
      "[29979.     0.     5.]\n",
      "[30194.     0.     1.]\n",
      "[30408.     0.     5.]\n",
      "[30622.     0.     1.]\n",
      "[30837.     0.     5.]\n",
      "[31051.     0.     1.]\n",
      "[31265.     0.     5.]\n",
      "[31479.     0.     1.]\n",
      "[31694.     0.     5.]\n",
      "[31801.     0.     1.]\n",
      "[31908.     0.     1.]\n",
      "[32123.     0.     5.]\n",
      "[32230.     0.     1.]\n",
      "[32337.     0.     1.]\n",
      "[32551.     0.     5.]\n",
      "[32658.     0.     1.]\n",
      "[32765.     0.     1.]\n",
      "[32980.     0.     5.]\n",
      "[33087.     0.     1.]\n",
      "[33194.     0.     1.]\n",
      "[33408.     0.     5.]\n",
      "[33515.     0.     1.]\n",
      "[33623.     0.     1.]\n",
      "[33837.     0.     5.]\n",
      "[33944.     0.     1.]\n",
      "[34051.     0.     1.]\n",
      "[34265.     0.     5.]\n",
      "[34372.     0.     1.]\n",
      "[34479.     0.     1.]\n",
      "[34694.     0.     5.]\n",
      "[34908.     0.     1.]\n",
      "[35122.     0.     5.]\n",
      "[35337.     0.     1.]\n",
      "[35551.     0.     5.]\n",
      "[35979.     0.     5.]\n",
      "[36194.     0.     1.]\n",
      "[36408.     0.     5.]\n",
      "[36622.     0.     1.]\n",
      "[36729.     0.     1.]\n",
      "[36837.     0.     5.]\n",
      "[37051.     0.     1.]\n",
      "[37158.     0.     1.]\n",
      "[37265.     0.     5.]\n",
      "[37479.     0.     1.]\n",
      "[37694.     0.     5.]\n",
      "[37908.     0.     1.]\n",
      "[38122.     0.     5.]\n",
      "[38337.     0.     1.]\n",
      "[38551.     0.     5.]\n",
      "[38658.     0.     1.]\n",
      "[38765.     0.     1.]\n",
      "[38979.     0.     5.]\n",
      "[39194.     0.     1.]\n",
      "[39408.     0.     5.]\n",
      "[39622.     0.     1.]\n",
      "[39837.     0.     5.]\n",
      "[40051.     0.     1.]\n",
      "[40265.     0.     5.]\n",
      "[40479.     0.     1.]\n",
      "[40587.     0.     1.]\n",
      "[40694.     0.     5.]\n",
      "[40908.     0.     1.]\n",
      "[41123.     0.     5.]\n",
      "[41337.     0.     1.]\n",
      "[41551.     0.     5.]\n",
      "[41765.     0.     1.]\n",
      "[41979.     0.     5.]\n",
      "[42194.     0.     1.]\n",
      "[42408.     0.     5.]\n",
      "[42837.     0.     5.]\n",
      "[43051.     0.     1.]\n",
      "[43265.     0.     5.]\n",
      "[43479.     0.     1.]\n",
      "[43587.     0.     1.]\n",
      "[43694.     0.     5.]\n",
      "[43908.     0.     1.]\n",
      "[44015.     0.     1.]\n",
      "[44122.     0.     5.]\n",
      "[44337.     0.     1.]\n",
      "[44551.     0.     5.]\n",
      "[44765.     0.     1.]\n",
      "[44979.     0.     5.]\n",
      "[45194.     0.     1.]\n",
      "[45408.     0.     5.]\n",
      "[45515.     0.     1.]\n",
      "[45622.     0.     1.]\n",
      "[45837.     0.     5.]\n",
      "[46051.     0.     1.]\n",
      "[46265.     0.     5.]\n",
      "[46479.     0.     1.]\n",
      "[46694.     0.     5.]\n",
      "[46908.     0.     1.]\n",
      "[47122.     0.     5.]\n",
      "[47337.     0.     1.]\n",
      "[47444.     0.     1.]\n",
      "[47551.     0.     5.]\n",
      "[47765.     0.     1.]\n",
      "[47979.     0.     5.]\n",
      "[48194.     0.     1.]\n",
      "[48408.     0.     5.]\n",
      "[48622.     0.     1.]\n",
      "[48837.     0.     5.]\n",
      "[49051.     0.     1.]\n",
      "[49265.     0.     5.]\n",
      "[49694.     0.     5.]\n",
      "[49908.     0.     1.]\n",
      "[50122.     0.     5.]\n",
      "[50337.     0.     1.]\n",
      "[50444.     0.     1.]\n",
      "[50551.     0.     5.]\n",
      "[50765.     0.     1.]\n",
      "[50872.     0.     1.]\n",
      "[50979.     0.     5.]\n",
      "[51194.     0.     1.]\n",
      "[51408.     0.     5.]\n",
      "[51622.     0.     1.]\n",
      "[51837.     0.     5.]\n",
      "[52051.     0.     1.]\n",
      "[52265.     0.     5.]\n",
      "[52372.     0.     1.]\n",
      "[52479.     0.     1.]\n",
      "[52694.     0.     5.]\n",
      "[52908.     0.     1.]\n",
      "[53122.     0.     5.]\n",
      "[53337.     0.     1.]\n",
      "[53551.     0.     5.]\n",
      "[53765.     0.     1.]\n",
      "[53979.     0.     5.]\n",
      "[54194.     0.     1.]\n",
      "[54301.     0.     1.]\n",
      "[54408.     0.     5.]\n",
      "[54622.     0.     1.]\n",
      "[54837.     0.     5.]\n",
      "[55051.     0.     1.]\n",
      "[55265.     0.     5.]\n",
      "[55479.     0.     1.]\n",
      "[55694.     0.     5.]\n",
      "[55908.     0.     1.]\n",
      "[56122.     0.     5.]\n",
      "[56551.     0.     5.]\n",
      "[56765.     0.     1.]\n",
      "[56979.     0.     5.]\n",
      "[57194.     0.     1.]\n",
      "[57301.     0.     1.]\n",
      "[57408.     0.     5.]\n",
      "[57622.     0.     1.]\n",
      "[57729.     0.     1.]\n",
      "[57837.     0.     5.]\n",
      "[58051.     0.     1.]\n",
      "[58265.     0.     5.]\n",
      "[58479.     0.     1.]\n",
      "[58694.     0.     5.]\n",
      "[58908.     0.     1.]\n",
      "[59122.     0.     5.]\n",
      "[59229.     0.     1.]\n",
      "[59337.     0.     1.]\n",
      "[59551.     0.     5.]\n",
      "[59765.     0.     1.]\n",
      "[59979.     0.     5.]\n",
      "[60194.     0.     1.]\n",
      "[60408.     0.     5.]\n",
      "[60622.     0.     1.]\n",
      "[60837.     0.     5.]\n",
      "[61051.     0.     1.]\n",
      "[61265.     0.     5.]\n",
      "[61479.     0.     1.]\n",
      "[61694.     0.     5.]\n",
      "[61908.     0.     1.]\n",
      "[62122.     0.     5.]\n",
      "[62337.     0.     1.]\n",
      "[62551.     0.     5.]\n",
      "[62765.     0.     1.]\n",
      "[62979.     0.     5.]\n",
      "[63194.     0.     1.]\n",
      "[63622.     0.     5.]\n",
      "[63837.     0.     1.]\n",
      "[64051.     0.     1.]\n",
      "[64265.     0.     5.]\n",
      "[64372.     0.     1.]\n",
      "[64479.     0.     1.]\n",
      "[64694.     0.     5.]\n",
      "[64908.     0.     1.]\n",
      "[65337.     0.     5.]\n",
      "[65551.     0.     1.]\n",
      "[65979.     0.     5.]\n",
      "[66194.     0.     1.]\n",
      "[66408.     0.     5.]\n",
      "[66622.     0.     1.]\n",
      "[67050.     0.     5.]\n",
      "[67265.     0.     1.]\n",
      "[67479.     0.     1.]\n",
      "[67694.     0.     5.]\n",
      "[67801.     0.     1.]\n",
      "[67908.     0.     1.]\n",
      "[68122.     0.     5.]\n",
      "[68337.     0.     1.]\n",
      "[68765.     0.     5.]\n",
      "[68979.     0.     1.]\n",
      "[69194.     0.     1.]\n",
      "[69408.     0.     5.]\n",
      "[69622.     0.     1.]\n",
      "[69837.     0.     5.]\n",
      "[70051.     0.     1.]\n",
      "[70479.     0.     5.]\n",
      "[70694.     0.     1.]\n",
      "[70908.     0.     1.]\n",
      "[71122.     0.     5.]\n",
      "[71230.     0.     1.]\n",
      "[71337.     0.     1.]\n",
      "[71551.     0.     5.]\n",
      "[71765.     0.     1.]\n",
      "[72194.     0.     5.]\n",
      "[72408.     0.     1.]\n",
      "[72837.     0.     5.]\n",
      "[73265.     0.     5.]\n",
      "[73479.     0.     1.]\n",
      "[74122.     0.     5.]\n",
      "[74337.     0.     1.]\n",
      "[74551.     0.     5.]\n",
      "[74658.     0.     1.]\n",
      "[74765.     0.     1.]\n",
      "[74872.     0.     1.]\n",
      "[74979.     0.     5.]\n",
      "[75087.     0.     1.]\n",
      "[75194.     0.     1.]\n",
      "[75301.     0.     1.]\n",
      "[75408.     0.     5.]\n",
      "[75515.     0.     1.]\n",
      "[75622.     0.     1.]\n",
      "[75729.     0.     1.]\n",
      "[75837.     0.     5.]\n",
      "[75944.     0.     1.]\n",
      "[76051.     0.     1.]\n",
      "[76158.     0.     1.]\n",
      "[76266.     0.     5.]\n",
      "[76479.     0.     1.]\n",
      "[76694.     0.     5.]\n",
      "[76908.     0.     1.]\n",
      "[77337.     0.     5.]\n",
      "[77550.     0.     1.]\n",
      "[77764.     0.     1.]\n",
      "[77979.     0.     5.]\n",
      "[78087.     0.     1.]\n",
      "[78194.     0.     1.]\n",
      "[78408.     0.     5.]\n",
      "[78622.     0.     1.]\n",
      "[79050.     0.     5.]\n",
      "[79265.     0.     1.]\n",
      "[79694.     0.     5.]\n",
      "[79907.     0.     1.]\n",
      "[80122.     0.     5.]\n",
      "[80337.     0.     1.]\n",
      "[80765.     0.     5.]\n",
      "[80979.     0.     1.]\n",
      "[81194.     0.     1.]\n",
      "[81408.     0.     5.]\n",
      "[81516.     0.     1.]\n",
      "[81623.     0.     1.]\n",
      "[81837.     0.     5.]\n",
      "[82051.     0.     1.]\n",
      "[82479.     0.     5.]\n",
      "[82694.     0.     1.]\n",
      "[82908.     0.     1.]\n",
      "[83122.     0.     5.]\n",
      "[83337.     0.     1.]\n",
      "[83551.     0.     5.]\n",
      "[83766.     0.     1.]\n",
      "[84194.     0.     5.]\n",
      "[84408.     0.     1.]\n",
      "[84622.     0.     1.]\n",
      "[84837.     0.     5.]\n",
      "[84944.     0.     1.]\n",
      "[85051.     0.     1.]\n",
      "[85265.     0.     5.]\n",
      "[85479.     0.     1.]\n",
      "[85908.     0.     5.]\n",
      "[86122.     0.     1.]\n",
      "[86551.     0.     5.]\n",
      "[86979.     0.     5.]\n",
      "[87193.     0.     1.]\n",
      "[87837.     0.     5.]\n",
      "[88051.     0.     1.]\n",
      "[88265.     0.     5.]\n",
      "[88372.     0.     1.]\n",
      "[88479.     0.     1.]\n",
      "[88586.     0.     1.]\n",
      "[88693.     0.     5.]\n",
      "[88801.     0.     1.]\n",
      "[88908.     0.     1.]\n",
      "[89015.     0.     1.]\n",
      "[89122.     0.     5.]\n",
      "[89229.     0.     1.]\n",
      "[89336.     0.     1.]\n",
      "[89443.     0.     1.]\n",
      "[89551.     0.     5.]\n",
      "[89658.     0.     1.]\n",
      "[89765.     0.     1.]\n",
      "[89872.     0.     1.]\n",
      "[89980.     0.     5.]\n",
      "[91051.     0.     5.]\n",
      "[91265.     0.     1.]\n",
      "[91479.     0.     1.]\n",
      "[92979.     0.     5.]\n",
      "[93194.     0.     1.]\n",
      "[95765.     0.     5.]\n",
      "[96194.     0.     1.]\n",
      "[96408.     0.     1.]\n",
      "[97908.     0.     5.]\n",
      "[98122.     0.     1.]\n",
      "[98336.     0.     1.]\n",
      "[99837.     0.     5.]\n",
      "[100051.      0.      1.]\n",
      "[103694.      0.      5.]\n",
      "[103908.      0.      1.]\n",
      "[104122.      0.      1.]\n",
      "[104551.      0.      5.]\n",
      "[104765.      0.      1.]\n",
      "[104979.      0.      1.]\n",
      "[105194.      0.      5.]\n",
      "[105301.      0.      1.]\n",
      "[105408.      0.      5.]\n",
      "[105515.      0.      1.]\n",
      "[105622.      0.      1.]\n",
      "[105729.      0.      1.]\n",
      "[105837.      0.      1.]\n",
      "[106051.      0.      5.]\n",
      "[106265.      0.      1.]\n",
      "[106694.      0.      1.]\n",
      "[107122.      0.      5.]\n",
      "[107229.      0.      1.]\n",
      "[107337.      0.      1.]\n",
      "[107551.      0.      5.]\n",
      "[107765.      0.      1.]\n",
      "[107979.      0.      1.]\n",
      "[108408.      0.      5.]\n",
      "[108622.      0.      1.]\n",
      "[108837.      0.      5.]\n",
      "[108944.      0.      1.]\n",
      "[109051.      0.      5.]\n",
      "[109158.      0.      1.]\n",
      "[109265.      0.      1.]\n",
      "[109479.      0.      1.]\n",
      "[109694.      0.      5.]\n",
      "[109908.      0.      1.]\n",
      "[110122.      0.      5.]\n",
      "[110336.      0.      1.]\n",
      "[110551.      0.      5.]\n",
      "[110765.      0.      1.]\n",
      "[110979.      0.      1.]\n",
      "[111408.      0.      5.]\n",
      "[111622.      0.      1.]\n",
      "[111836.      0.      1.]\n",
      "[112051.      0.      5.]\n",
      "[112158.      0.      1.]\n",
      "[112265.      0.      5.]\n",
      "[112372.      0.      1.]\n",
      "[112479.      0.      1.]\n",
      "[112586.      0.      1.]\n",
      "[112694.      0.      1.]\n",
      "[112908.      0.      5.]\n",
      "[113122.      0.      1.]\n",
      "[113551.      0.      1.]\n",
      "[113979.      0.      5.]\n",
      "[114087.      0.      1.]\n",
      "[114194.      0.      1.]\n",
      "[114407.      0.      5.]\n",
      "[114622.      0.      1.]\n",
      "[114837.      0.      1.]\n",
      "[115265.      0.      5.]\n",
      "[115479.      0.      1.]\n",
      "[115694.      0.      5.]\n",
      "[116337.      0.      1.]\n",
      "[116551.      0.      5.]\n",
      "[116765.      0.      1.]\n",
      "[116980.      0.      5.]\n",
      "[117194.      0.      1.]\n",
      "[117408.      0.      5.]\n",
      "[117837.      0.      1.]\n",
      "[118051.      0.      1.]\n",
      "[118265.      0.      5.]\n",
      "[118479.      0.      1.]\n",
      "[118694.      0.      1.]\n",
      "[118908.      0.      1.]\n",
      "[119122.      0.      5.]\n",
      "[119229.      0.      1.]\n",
      "[119551.      0.      1.]\n",
      "[119765.      0.      5.]\n",
      "[119979.      0.      1.]\n",
      "[120408.      0.      1.]\n",
      "[120837.      0.      5.]\n",
      "[121265.      0.      1.]\n",
      "[121479.      0.      1.]\n",
      "[121695.      0.      5.]\n",
      "[122122.      0.      1.]\n",
      "[122337.      0.      1.]\n",
      "[122551.      0.      5.]\n",
      "[122658.      0.      1.]\n",
      "[122979.      0.      1.]\n",
      "[123194.      0.      1.]\n",
      "[123408.      0.      5.]\n",
      "[123622.      0.      1.]\n",
      "[123837.      0.      5.]\n",
      "[124051.      0.      1.]\n",
      "[124265.      0.      5.]\n",
      "[124694.      0.      1.]\n",
      "[124908.      0.      1.]\n",
      "[125122.      0.      5.]\n",
      "[125337.      0.      1.]\n",
      "[125550.      0.      1.]\n",
      "[125765.      0.      1.]\n",
      "[125979.      0.      5.]\n",
      "[126087.      0.      1.]\n",
      "[126408.      0.      1.]\n",
      "[126622.      0.      5.]\n",
      "[126837.      0.      1.]\n",
      "[127265.      0.      1.]\n",
      "[127694.      0.      5.]\n",
      "[128122.      0.      1.]\n",
      "[128336.      0.      1.]\n",
      "[128551.      0.      5.]\n",
      "[128979.      0.      1.]\n",
      "[129194.      0.      1.]\n",
      "[129408.      0.      5.]\n",
      "[129515.      0.      1.]\n",
      "[129837.      0.      1.]\n",
      "[130051.      0.      5.]\n",
      "[130265.      0.      1.]\n",
      "[130694.      0.      1.]\n",
      "[131122.      0.      5.]\n",
      "[131551.      0.      5.]\n",
      "[131765.      0.      1.]\n",
      "[131979.      0.      1.]\n",
      "[132408.      0.      5.]\n",
      "[132622.      0.      1.]\n",
      "[132837.      0.      5.]\n",
      "[132944.      0.      1.]\n",
      "[133265.      0.      5.]\n",
      "[133479.      0.      1.]\n",
      "[133694.      0.      5.]\n",
      "[134122.      0.      1.]\n",
      "[134551.      0.      5.]\n",
      "[134979.      0.      1.]\n",
      "[135194.      0.      1.]\n",
      "[135407.      0.      5.]\n",
      "[135837.      0.      1.]\n",
      "[136052.      0.      1.]\n",
      "[136265.      0.      5.]\n",
      "[136372.      0.      1.]\n",
      "[136694.      0.      1.]\n",
      "[136908.      0.      5.]\n",
      "[137122.      0.      1.]\n",
      "[137551.      0.      1.]\n",
      "[137979.      0.      5.]\n",
      "[138408.      0.      5.]\n",
      "[138623.      0.      1.]\n",
      "[138837.      0.      1.]\n",
      "[139265.      0.      5.]\n",
      "[139480.      0.      1.]\n",
      "[139694.      0.      5.]\n",
      "[139801.      0.      1.]\n",
      "[140122.      0.      5.]\n",
      "[140337.      0.      1.]\n",
      "[140551.      0.      5.]\n",
      "[140979.      0.      1.]\n",
      "[141408.      0.      5.]\n",
      "[141837.      0.      1.]\n",
      "[142051.      0.      1.]\n",
      "[142265.      0.      5.]\n",
      "[142694.      0.      1.]\n",
      "[142908.      0.      1.]\n",
      "[143122.      0.      5.]\n",
      "[143229.      0.      1.]\n",
      "[143337.      0.      1.]\n",
      "[143551.      0.      1.]\n",
      "[143765.      0.      1.]\n",
      "[143979.      0.      5.]\n",
      "[144087.      0.      1.]\n",
      "[144194.      0.      1.]\n",
      "[144301.      0.      1.]\n",
      "[144408.      0.      1.]\n",
      "[144622.      0.      1.]\n",
      "[144837.      0.      5.]\n",
      "[145265.      0.      1.]\n",
      "[145694.      0.      1.]\n",
      "[146122.      0.      5.]\n",
      "[146337.      0.      1.]\n",
      "[146765.      0.      1.]\n",
      "[147194.      0.      1.]\n",
      "[147408.      0.      5.]\n",
      "[147837.      0.      1.]\n",
      "[148265.      0.      5.]\n",
      "[148694.      0.      1.]\n",
      "[149123.      0.      1.]\n",
      "[149551.      0.      5.]\n",
      "[149765.      0.      1.]\n",
      "[150194.      0.      1.]\n",
      "[150622.      0.      1.]\n",
      "[150837.      0.      5.]\n",
      "[151265.      0.      1.]\n",
      "[151694.      0.      5.]\n",
      "[151908.      0.      1.]\n",
      "[152015.      0.      1.]\n",
      "[152122.      0.      5.]\n",
      "[152337.      0.      1.]\n",
      "[152551.      0.      5.]\n",
      "[152766.      0.      1.]\n",
      "[152979.      0.      1.]\n",
      "[153194.      0.      1.]\n",
      "[153301.      0.      1.]\n",
      "[153408.      0.      5.]\n",
      "[153516.      0.      1.]\n",
      "[153623.      0.      1.]\n",
      "[153730.      0.      1.]\n",
      "[153837.      0.      1.]\n",
      "[154051.      0.      1.]\n",
      "[154265.      0.      5.]\n",
      "[154479.      0.      1.]\n",
      "[154694.      0.      5.]\n",
      "[154908.      0.      1.]\n",
      "[155122.      0.      5.]\n",
      "[155229.      0.      1.]\n",
      "[155337.      0.      1.]\n",
      "[155551.      0.      5.]\n",
      "[155765.      0.      1.]\n",
      "[155979.      0.      5.]\n",
      "[156194.      0.      1.]\n",
      "[156408.      0.      5.]\n",
      "[156622.      0.      1.]\n",
      "[156837.      0.      1.]\n",
      "[156944.      0.      1.]\n",
      "[157051.      0.      5.]\n",
      "[157158.      0.      1.]\n",
      "[157265.      0.      1.]\n",
      "[157479.      0.      1.]\n",
      "[157694.      0.      5.]\n",
      "[157908.      0.      1.]\n",
      "[158122.      0.      5.]\n",
      "[158336.      0.      1.]\n",
      "[158551.      0.      5.]\n",
      "[158765.      0.      1.]\n",
      "[158872.      0.      1.]\n",
      "[158979.      0.      5.]\n",
      "[159193.      0.      1.]\n",
      "[159408.      0.      5.]\n",
      "[159622.      0.      1.]\n",
      "[159837.      0.      5.]\n",
      "[160051.      0.      1.]\n",
      "[160158.      0.      1.]\n",
      "[160265.      0.      5.]\n",
      "[160373.      0.      1.]\n",
      "[160480.      0.      1.]\n",
      "[160587.      0.      1.]\n",
      "[160694.      0.      1.]\n",
      "[160908.      0.      1.]\n",
      "[161122.      0.      5.]\n",
      "[161337.      0.      1.]\n",
      "[161551.      0.      5.]\n",
      "[161766.      0.      1.]\n",
      "[161979.      0.      5.]\n",
      "[162087.      0.      1.]\n",
      "[162194.      0.      1.]\n",
      "[162408.      0.      5.]\n",
      "[162622.      0.      1.]\n",
      "[162837.      0.      5.]\n",
      "[163051.      0.      1.]\n",
      "[163265.      0.      5.]\n",
      "[163479.      0.      1.]\n",
      "[163694.      0.      5.]\n",
      "[163907.      0.      5.]\n",
      "[164015.      0.      1.]\n",
      "[164123.      0.      1.]\n",
      "[164337.      0.      1.]\n",
      "[164551.      0.      5.]\n",
      "[164765.      0.      1.]\n",
      "[164979.      0.      5.]\n",
      "[165193.      0.      1.]\n",
      "[165408.      0.      5.]\n",
      "[165622.      0.      1.]\n",
      "[165837.      0.      5.]\n",
      "[166051.      0.      1.]\n",
      "[166265.      0.      5.]\n",
      "[166479.      0.      5.]\n",
      "[166694.      0.      1.]\n",
      "[166908.      0.      5.]\n",
      "[167015.      0.      1.]\n",
      "[167122.      0.      5.]\n",
      "[167230.      0.      1.]\n",
      "[167337.      0.      1.]\n",
      "[167445.      0.      1.]\n",
      "[167551.      0.      1.]\n",
      "[167765.      0.      1.]\n",
      "[167979.      0.      5.]\n",
      "[168194.      0.      1.]\n",
      "[168408.      0.      1.]\n",
      "[168622.      0.      1.]\n",
      "[168837.      0.      5.]\n",
      "[168944.      0.      1.]\n",
      "[169052.      0.      1.]\n",
      "[169265.      0.      5.]\n",
      "[169479.      0.      1.]\n",
      "[169694.      0.      5.]\n",
      "[169908.      0.      1.]\n",
      "[170122.      0.      5.]\n",
      "[170337.      0.      1.]\n",
      "[170551.      0.      5.]\n",
      "[172265.      0.      5.]\n",
      "[172479.      0.      1.]\n",
      "[172694.      0.      5.]\n",
      "[172908.      0.      1.]\n",
      "[173122.      0.      5.]\n",
      "[173336.      0.      1.]\n",
      "[173550.      0.      5.]\n",
      "[173765.      0.      1.]\n",
      "[173872.      0.      1.]\n",
      "[173979.      0.      5.]\n",
      "[174087.      0.      1.]\n",
      "[174194.      0.      1.]\n",
      "[174302.      0.      1.]\n",
      "[174408.      0.      1.]\n",
      "[174622.      0.      5.]\n",
      "[174837.      0.      1.]\n",
      "[175051.      0.      1.]\n",
      "[175265.      0.      1.]\n",
      "[175479.      0.      1.]\n",
      "[175694.      0.      5.]\n",
      "[175801.      0.      1.]\n",
      "[175908.      0.      1.]\n",
      "[176122.      0.      5.]\n",
      "[176337.      0.      1.]\n",
      "[176551.      0.      1.]\n",
      "[176766.      0.      1.]\n",
      "[176979.      0.      5.]\n",
      "[177193.      0.      1.]\n",
      "[177408.      0.      1.]\n",
      "[177515.      0.      1.]\n",
      "[177622.      0.      5.]\n",
      "[177729.      0.      1.]\n",
      "[177837.      0.      1.]\n",
      "[178051.      0.      1.]\n",
      "[178265.      0.      5.]\n",
      "[178479.      0.      1.]\n",
      "[178694.      0.      5.]\n",
      "[178908.      0.      1.]\n",
      "[179122.      0.      5.]\n",
      "[179337.      0.      1.]\n",
      "[179551.      0.      5.]\n",
      "[179766.      0.      1.]\n",
      "[179979.      0.      5.]\n",
      "[180194.      0.      5.]\n",
      "[180408.      0.      1.]\n",
      "[180622.      0.      5.]\n",
      "[180729.      0.      1.]\n",
      "[180836.      0.      5.]\n",
      "[180944.      0.      1.]\n",
      "[181051.      0.      1.]\n",
      "[181159.      0.      1.]\n",
      "[181265.      0.      1.]\n",
      "[181479.      0.      1.]\n",
      "[181694.      0.      5.]\n",
      "[181908.      0.      1.]\n",
      "[182122.      0.      1.]\n",
      "[182336.      0.      1.]\n",
      "[182551.      0.      5.]\n",
      "[182658.      0.      1.]\n",
      "[182766.      0.      1.]\n",
      "[182979.      0.      5.]\n",
      "[183194.      0.      1.]\n",
      "[183408.      0.      5.]\n",
      "[183623.      0.      1.]\n",
      "[183837.      0.      5.]\n",
      "[184051.      0.      1.]\n",
      "[184265.      0.      5.]\n",
      "[184372.      0.      1.]\n",
      "[184480.      0.      5.]\n",
      "[184587.      0.      1.]\n",
      "[184695.      0.      1.]\n",
      "[184908.      0.      5.]\n",
      "[185122.      0.      1.]\n",
      "[185337.      0.      5.]\n",
      "[185551.      0.      1.]\n",
      "[185765.      0.      1.]\n",
      "[187051.      0.      5.]\n",
      "[187265.      0.      1.]\n",
      "[187479.      0.      1.]\n",
      "[188979.      0.      5.]\n",
      "[189194.      0.      1.]\n",
      "[189408.      0.      5.]\n",
      "[189515.      0.      1.]\n",
      "[189622.      0.      1.]\n",
      "[189837.      0.      5.]\n",
      "[189944.      0.      1.]\n",
      "[190051.      0.      1.]\n",
      "[190265.      0.      5.]\n",
      "[190372.      0.      1.]\n",
      "[190479.      0.      1.]\n",
      "[190694.      0.      5.]\n",
      "[190801.      0.      1.]\n",
      "[190908.      0.      1.]\n",
      "[191122.      0.      5.]\n",
      "[191229.      0.      1.]\n",
      "[191337.      0.      1.]\n",
      "[191551.      0.      5.]\n",
      "[191658.      0.      1.]\n",
      "[191765.      0.      1.]\n",
      "[191979.      0.      5.]\n",
      "[192194.      0.      1.]\n",
      "[192408.      0.      1.]\n",
      "[192622.      0.      1.]\n",
      "[192837.      0.      5.]\n",
      "[193051.      0.      1.]\n",
      "[193265.      0.      5.]\n",
      "[193479.      0.      1.]\n",
      "[193694.      0.      5.]\n",
      "[193908.      0.      1.]\n",
      "[194122.      0.      5.]\n",
      "[194337.      0.      1.]\n",
      "[194551.      0.      5.]\n",
      "[194658.      0.      1.]\n",
      "[194765.      0.      1.]\n",
      "[194979.      0.      1.]\n",
      "[195194.      0.      5.]\n",
      "[195301.      0.      1.]\n",
      "[195408.      0.      1.]\n",
      "[195515.      0.      1.]\n",
      "[195622.      0.      1.]\n",
      "[195729.      0.      1.]\n",
      "[195837.      0.      1.]\n",
      "[195944.      0.      1.]\n",
      "[196051.      0.      1.]\n",
      "[196158.      0.      1.]\n",
      "[196265.      0.      5.]\n",
      "[196479.      0.      1.]\n",
      "[196694.      0.      5.]\n",
      "[196909.      0.      1.]\n",
      "[197122.      0.      5.]\n",
      "[197337.      0.      1.]\n",
      "[197551.      0.      5.]\n",
      "[197766.      0.      1.]\n",
      "[197979.      0.      5.]\n",
      "[198087.      0.      1.]\n",
      "[198194.      0.      1.]\n",
      "[198301.      0.      5.]\n",
      "[198408.      0.      1.]\n",
      "[198622.      0.      5.]\n",
      "[198729.      0.      1.]\n",
      "[198836.      0.      1.]\n",
      "[198943.      0.      1.]\n",
      "[199050.      0.      1.]\n",
      "[199157.      0.      1.]\n",
      "[199265.      0.      1.]\n",
      "[199372.      0.      1.]\n",
      "[199479.      0.      1.]\n",
      "[199586.      0.      1.]\n",
      "[199695.      0.      5.]\n",
      "[199909.      0.      1.]\n",
      "[200123.      0.      5.]\n",
      "[200337.      0.      1.]\n",
      "[200551.      0.      5.]\n",
      "[200765.      0.      1.]\n",
      "[200979.      0.      5.]\n",
      "[201194.      0.      1.]\n",
      "[201409.      0.      5.]\n",
      "[201515.      0.      1.]\n",
      "[201622.      0.      1.]\n",
      "[201837.      0.      1.]\n",
      "[202051.      0.      5.]\n",
      "[202158.      0.      1.]\n",
      "[202265.      0.      1.]\n",
      "[202372.      0.      1.]\n",
      "[202480.      0.      1.]\n",
      "[202587.      0.      1.]\n",
      "[202694.      0.      1.]\n",
      "[202801.      0.      1.]\n",
      "[202908.      0.      1.]\n",
      "[203015.      0.      1.]\n",
      "[203122.      0.      5.]\n",
      "[203337.      0.      1.]\n",
      "[203551.      0.      5.]\n",
      "[203764.      0.      1.]\n",
      "[203872.      0.      1.]\n",
      "[203979.      0.      1.]\n",
      "[204194.      0.      1.]\n",
      "[204408.      0.      5.]\n",
      "[204622.      0.      1.]\n",
      "[204837.      0.      5.]\n",
      "[204944.      0.      1.]\n",
      "[205051.      0.      1.]\n",
      "[205158.      0.      1.]\n",
      "[205265.      0.      1.]\n",
      "[205372.      0.      1.]\n",
      "[205479.      0.      1.]\n",
      "[205587.      0.      1.]\n",
      "[205694.      0.      1.]\n",
      "[205801.      0.      1.]\n",
      "[205908.      0.      1.]\n",
      "[206016.      0.      1.]\n",
      "[206122.      0.      1.]\n",
      "[206229.      0.      1.]\n",
      "[206336.      0.      1.]\n",
      "[206443.      0.      1.]\n",
      "[206551.      0.      5.]\n",
      "[206765.      0.      1.]\n",
      "[206979.      0.      5.]\n",
      "[207193.      0.      1.]\n",
      "[207408.      0.      5.]\n",
      "[207622.      0.      1.]\n",
      "[207837.      0.      5.]\n",
      "[208051.      0.      1.]\n",
      "[208265.      0.      5.]\n",
      "[208479.      0.      1.]\n",
      "[208694.      0.      5.]\n",
      "[208908.      0.      1.]\n",
      "[209122.      0.      5.]\n",
      "[209336.      0.      1.]\n",
      "[209551.      0.      1.]\n",
      "[209979.      0.      5.]\n",
      "[210194.      0.      1.]\n",
      "[210408.      0.      5.]\n",
      "[210622.      0.      1.]\n",
      "[210837.      0.      1.]\n",
      "[211265.      0.      5.]\n",
      "[211479.      0.      1.]\n",
      "[211694.      0.      5.]\n",
      "[211802.      0.      1.]\n",
      "[211909.      0.      1.]\n",
      "[212016.      0.      5.]\n",
      "[212123.      0.      1.]\n",
      "[212337.      0.      5.]\n",
      "[212444.      0.      1.]\n",
      "[212551.      0.      1.]\n",
      "[212658.      0.      1.]\n",
      "[212765.      0.      1.]\n",
      "[212872.      0.      1.]\n",
      "[212979.      0.      1.]\n",
      "[213087.      0.      1.]\n",
      "[213194.      0.      1.]\n",
      "[213301.      0.      1.]\n",
      "[213408.      0.      5.]\n",
      "[213622.      0.      1.]\n",
      "[213837.      0.      5.]\n",
      "[214051.      0.      1.]\n",
      "[214265.      0.      5.]\n",
      "[214480.      0.      1.]\n",
      "[214694.      0.      5.]\n",
      "[214908.      0.      1.]\n",
      "[215122.      0.      5.]\n",
      "[215336.      0.      1.]\n",
      "[215550.      0.      5.]\n",
      "[215764.      0.      1.]\n",
      "[215979.      0.      5.]\n",
      "[216194.      0.      1.]\n",
      "[216408.      0.      1.]\n",
      "[216622.      0.      5.]\n",
      "[216729.      0.      1.]\n",
      "[216837.      0.      1.]\n",
      "[217051.      0.      1.]\n",
      "[217265.      0.      1.]\n",
      "[217478.      0.      5.]\n",
      "[217587.      0.      1.]\n",
      "[217693.      0.      1.]\n",
      "[217908.      0.      1.]\n",
      "[218122.      0.      1.]\n",
      "[218337.      0.      5.]\n",
      "[218444.      0.      1.]\n",
      "[218551.      0.      1.]\n",
      "[218658.      0.      1.]\n",
      "[218765.      0.      1.]\n",
      "[218979.      0.      5.]\n",
      "[219194.      0.      1.]\n",
      "[219408.      0.      5.]\n",
      "[219515.      0.      1.]\n",
      "[219622.      0.      5.]\n",
      "[219729.      0.      1.]\n",
      "[219837.      0.      5.]\n",
      "[219890.      0.      5.]\n",
      "[219944.      0.      5.]\n",
      "[219997.      0.      5.]\n",
      "[220051.      0.      5.]\n",
      "[220104.      0.      5.]\n",
      "[220158.      0.      5.]\n",
      "[220212.      0.      5.]\n",
      "[220265.      0.      5.]\n"
     ]
    }
   ],
   "source": [
    "for b in beatmap:\n",
    "    print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset,num_of_batches,processed_audio_list,processed_beatmap_list,hyper_params_list = new_process_audio_and_beatmap_for_model_all_and_save(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_of_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11,)\n",
      "(11,)\n",
      "(11,)\n",
      "(11,)\n",
      "(11,)\n",
      "(11,)\n",
      "(11,)\n",
      "(11,)\n",
      "(11,)\n",
      "(11,)\n",
      "(11,)\n",
      "(11,)\n",
      "(11,)\n",
      "(11,)\n",
      "(11,)\n",
      "(11,)\n",
      "(11,)\n",
      "(11,)\n",
      "(11,)\n",
      "(11,)\n"
     ]
    }
   ],
   "source": [
    "for a,b,p in dataset:\n",
    "    print(p.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for h in processed_beatmap_list[19]:\n",
    "#     print(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_9 (InputLayer)           [(None, 60000, 14)]  0           []                               \n",
      "                                                                                                  \n",
      " dense_44 (Dense)               (None, 60000, 128)   1920        ['input_9[0][0]']                \n",
      "                                                                                                  \n",
      " positional_encoding_4 (Positio  (None, 60000, 128)  0           ['dense_44[0][0]']               \n",
      " nalEncoding)                                                                                     \n",
      "                                                                                                  \n",
      " multi_head_attention_16 (Multi  (None, 60000, 128)  527488      ['positional_encoding_4[0][0]',  \n",
      " HeadAttention)                                                   'positional_encoding_4[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_36 (Dropout)           (None, 60000, 128)   0           ['multi_head_attention_16[0][0]']\n",
      "                                                                                                  \n",
      " tf.__operators__.add_32 (TFOpL  (None, 60000, 128)  0           ['positional_encoding_4[0][0]',  \n",
      " ambda)                                                           'dropout_36[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_32 (LayerN  (None, 60000, 128)  256         ['tf.__operators__.add_32[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " sequential_16 (Sequential)     (None, 60000, 128)   131712      ['layer_normalization_32[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_37 (Dropout)           (None, 60000, 128)   0           ['sequential_16[0][0]']          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_33 (TFOpL  (None, 60000, 128)  0           ['layer_normalization_32[0][0]', \n",
      " ambda)                                                           'dropout_37[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_33 (LayerN  (None, 60000, 128)  256         ['tf.__operators__.add_33[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_17 (Multi  (None, 60000, 128)  527488      ['layer_normalization_33[0][0]', \n",
      " HeadAttention)                                                   'layer_normalization_33[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_38 (Dropout)           (None, 60000, 128)   0           ['multi_head_attention_17[0][0]']\n",
      "                                                                                                  \n",
      " tf.__operators__.add_34 (TFOpL  (None, 60000, 128)  0           ['layer_normalization_33[0][0]', \n",
      " ambda)                                                           'dropout_38[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_34 (LayerN  (None, 60000, 128)  256         ['tf.__operators__.add_34[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " sequential_17 (Sequential)     (None, 60000, 128)   131712      ['layer_normalization_34[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_39 (Dropout)           (None, 60000, 128)   0           ['sequential_17[0][0]']          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_35 (TFOpL  (None, 60000, 128)  0           ['layer_normalization_34[0][0]', \n",
      " ambda)                                                           'dropout_39[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_35 (LayerN  (None, 60000, 128)  256         ['tf.__operators__.add_35[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_18 (Multi  (None, 60000, 128)  527488      ['layer_normalization_35[0][0]', \n",
      " HeadAttention)                                                   'layer_normalization_35[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_40 (Dropout)           (None, 60000, 128)   0           ['multi_head_attention_18[0][0]']\n",
      "                                                                                                  \n",
      " tf.__operators__.add_36 (TFOpL  (None, 60000, 128)  0           ['layer_normalization_35[0][0]', \n",
      " ambda)                                                           'dropout_40[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_36 (LayerN  (None, 60000, 128)  256         ['tf.__operators__.add_36[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " sequential_18 (Sequential)     (None, 60000, 128)   131712      ['layer_normalization_36[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_41 (Dropout)           (None, 60000, 128)   0           ['sequential_18[0][0]']          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_37 (TFOpL  (None, 60000, 128)  0           ['layer_normalization_36[0][0]', \n",
      " ambda)                                                           'dropout_41[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_37 (LayerN  (None, 60000, 128)  256         ['tf.__operators__.add_37[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_19 (Multi  (None, 60000, 128)  527488      ['layer_normalization_37[0][0]', \n",
      " HeadAttention)                                                   'layer_normalization_37[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_42 (Dropout)           (None, 60000, 128)   0           ['multi_head_attention_19[0][0]']\n",
      "                                                                                                  \n",
      " tf.__operators__.add_38 (TFOpL  (None, 60000, 128)  0           ['layer_normalization_37[0][0]', \n",
      " ambda)                                                           'dropout_42[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_38 (LayerN  (None, 60000, 128)  256         ['tf.__operators__.add_38[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " sequential_19 (Sequential)     (None, 60000, 128)   131712      ['layer_normalization_38[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_43 (Dropout)           (None, 60000, 128)   0           ['sequential_19[0][0]']          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_39 (TFOpL  (None, 60000, 128)  0           ['layer_normalization_38[0][0]', \n",
      " ambda)                                                           'dropout_43[0][0]']             \n",
      "                                                                                                  \n",
      " input_10 (InputLayer)          [(None, 11)]         0           []                               \n",
      "                                                                                                  \n",
      " layer_normalization_39 (LayerN  (None, 60000, 128)  256         ['tf.__operators__.add_39[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " repeat_vector_1 (RepeatVector)  (None, 60000, 11)   0           ['input_10[0][0]']               \n",
      "                                                                                                  \n",
      " concatenate_4 (Concatenate)    (None, 60000, 139)   0           ['layer_normalization_39[0][0]', \n",
      "                                                                  'repeat_vector_1[0][0]']        \n",
      "                                                                                                  \n",
      " dense_53 (Dense)               (None, 60000, 256)   35840       ['concatenate_4[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_44 (Dropout)           (None, 60000, 256)   0           ['dense_53[0][0]']               \n",
      "                                                                                                  \n",
      " dense_54 (Dense)               (None, 60000, 4)     1028        ['dropout_44[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,677,636\n",
      "Trainable params: 2,677,636\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"c:\\Users\\darsh\\anaconda3\\envs\\latestEnv\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\darsh\\anaconda3\\envs\\latestEnv\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\darsh\\anaconda3\\envs\\latestEnv\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\darsh\\anaconda3\\envs\\latestEnv\\lib\\site-packages\\keras\\engine\\training.py\", line 993, in train_step\n        y_pred = self(x, training=True)\n    File \"c:\\Users\\darsh\\anaconda3\\envs\\latestEnv\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"c:\\Users\\darsh\\anaconda3\\envs\\latestEnv\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 216, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Layer \"model_1\" expects 2 input(s), but it received 1 input tensors. Inputs received: [<tf.Tensor 'IteratorGetNext:0' shape=(60000, 14) dtype=float32>]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 91\u001b[0m\n\u001b[0;32m     88\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m prepare_dataset(dataset)\n\u001b[0;32m     90\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 91\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\darsh\\anaconda3\\envs\\latestEnv\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filedww2puaw.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"c:\\Users\\darsh\\anaconda3\\envs\\latestEnv\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\darsh\\anaconda3\\envs\\latestEnv\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\darsh\\anaconda3\\envs\\latestEnv\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\darsh\\anaconda3\\envs\\latestEnv\\lib\\site-packages\\keras\\engine\\training.py\", line 993, in train_step\n        y_pred = self(x, training=True)\n    File \"c:\\Users\\darsh\\anaconda3\\envs\\latestEnv\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"c:\\Users\\darsh\\anaconda3\\envs\\latestEnv\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 216, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Layer \"model_1\" expects 2 input(s), but it received 1 input tensors. Inputs received: [<tf.Tensor 'IteratorGetNext:0' shape=(60000, 14) dtype=float32>]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "class PositionalEncoding(keras.layers.Layer):\n",
    "    def __init__(self, position, d_model):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.pos_encoding = self.positional_encoding(position, d_model)\n",
    "\n",
    "    def get_angles(self, position, i, d_model):\n",
    "        angles = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32))\n",
    "        return position * angles\n",
    "\n",
    "    def positional_encoding(self, position, d_model):\n",
    "        angle_rads = self.get_angles(\n",
    "            position=tf.range(position, dtype=tf.float32)[:, tf.newaxis],\n",
    "            i=tf.range(d_model, dtype=tf.float32)[tf.newaxis, :],\n",
    "            d_model=d_model\n",
    "        )\n",
    "        sines = tf.math.sin(angle_rads[:, 0::2])\n",
    "        cosines = tf.math.cos(angle_rads[:, 1::2])\n",
    "        pos_encoding = tf.concat([sines, cosines], axis=-1)\n",
    "        pos_encoding = pos_encoding[tf.newaxis, ...]\n",
    "        return tf.cast(pos_encoding, tf.float32)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]\n",
    "\n",
    "def create_transformer_model(input_shape, output_shape, params_shape, num_layers=4, d_model=128, num_heads=8, dff=512, dropout_rate=0.1):\n",
    "    # Input layers\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    params_input = keras.Input(shape=params_shape)\n",
    "\n",
    "    # Embedding layer for input\n",
    "    embedding = keras.layers.Dense(d_model)(inputs)\n",
    "    \n",
    "    # Add positional encoding\n",
    "    pos_encoding = PositionalEncoding(input_shape[0], d_model)\n",
    "    x = pos_encoding(embedding)\n",
    "\n",
    "    # Transformer layers\n",
    "    for _ in range(num_layers):\n",
    "        # Multi-head attention\n",
    "        attention = keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model)\n",
    "        attn_output = attention(x, x)\n",
    "        attn_output = keras.layers.Dropout(dropout_rate)(attn_output)\n",
    "        out1 = keras.layers.LayerNormalization(epsilon=1e-6)(x + attn_output)\n",
    "\n",
    "        # Feed-forward network\n",
    "        ffn = keras.Sequential([\n",
    "            keras.layers.Dense(dff, activation='relu'),\n",
    "            keras.layers.Dense(d_model)\n",
    "        ])\n",
    "        ffn_output = ffn(out1)\n",
    "        ffn_output = keras.layers.Dropout(dropout_rate)(ffn_output)\n",
    "        x = keras.layers.LayerNormalization(epsilon=1e-6)(out1 + ffn_output)\n",
    "\n",
    "    # Concatenate with params input (broadcasting params across time steps)\n",
    "    params_broadcasted = keras.layers.RepeatVector(input_shape[0])(params_input)\n",
    "    x = keras.layers.Concatenate()([x, params_broadcasted])\n",
    "\n",
    "    # Final dense layers\n",
    "    x = keras.layers.Dense(256, activation='relu')(x)\n",
    "    x = keras.layers.Dropout(dropout_rate)(x)\n",
    "    outputs = keras.layers.Dense(output_shape[1])(x)\n",
    "\n",
    "    # Create model\n",
    "    model = keras.Model(inputs=[inputs, params_input], outputs=outputs)\n",
    "    return model\n",
    "\n",
    "# Create and compile the model\n",
    "input_shape = (60000, 14)\n",
    "output_shape = (60000, 4)\n",
    "params_shape = (11,)\n",
    "\n",
    "model = create_transformer_model(input_shape, output_shape, params_shape)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "# Print model summary\n",
    "model.summary()\n",
    "\n",
    "# Function to prepare the dataset for training\n",
    "def prepare_dataset(dataset):\n",
    "    return dataset.map(lambda x, y, z: ((x, z), y)).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Create and prepare the dataset\n",
    "train_dataset = prepare_dataset(dataset)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(dataset, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from makeBeatmap import make_beatmap_from_output\n",
    "# make_beatmap_from_output(df.iloc[20],processed_beatmap_list[20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for input_data, output_data in dataset.take(1):\n",
    "#     print(\"First input:\")\n",
    "#     print(input_data.numpy()[0][0])\n",
    "#     print(\"First output:\")\n",
    "#     print(output_data.numpy()[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import tensorflow as tf\n",
    "\n",
    "# model = tf.keras.Sequential([\n",
    "#     tf.keras.layers.Input(shape=(None, 142)),  # Input layer for variable length sequences with 142 features\n",
    "#     tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(256 , return_sequences=True)),\n",
    "#     tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(256 , return_sequences=True)),\n",
    "#     # tf.keras.layers.Dense(64, activation='relu'),  # Optional dense layer\n",
    "#     tf.keras.layers.Dense(35)  # Output layer, size 35 to match your output data dimension\n",
    "# ])\n",
    "\n",
    "# model.compile(optimizer='adam', loss=tf.keras.losses.Huber())\n",
    "\n",
    "# # Training the model\n",
    "# model.fit(dataset, epochs=10, steps_per_epoch=num_of_batches) # instead of 50 make it number of songs in final processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "\n",
    "# # Check if GPU is available\n",
    "# gpus = tf.config.experimental.list_physical_devices()\n",
    "# gpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "\n",
    "# # # Ensure TensorFlow is set up to use the GPU\n",
    "# # physical_devices = tf.config.list_physical_devices('GPU')\n",
    "# # if len(physical_devices) > 0:\n",
    "# #     tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "# #     print(\"GPU is available and configured.\")\n",
    "# # else:\n",
    "# #     print(\"GPU is not available.\")\n",
    "\n",
    "# # Check for GPU availability\n",
    "# print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "# def custom_loss(y_true, y_pred):\n",
    "#     hit_weight = 5.0  # Higher weight for hit objects\n",
    "#     no_hit_weight = 1.0  # Lower weight for no hit objects (zeros)\n",
    "    \n",
    "#     # Compute the Mean Squared Error (MSE) loss\n",
    "#     loss = tf.keras.losses.mean_squared_error(y_true, y_pred)  # Shape: (batch_size, time_steps, output_dim)\n",
    "    \n",
    "#     # Create a weight matrix based on the true values (broadcast across batch and time steps)\n",
    "#     weights = tf.where(tf.not_equal(y_true, 0), hit_weight, no_hit_weight)  # Shape: (batch_size, time_steps, output_dim)\n",
    "    \n",
    "#     # Ensure the weights have the same shape as the loss\n",
    "#     weights = tf.cast(weights, tf.float32)\n",
    "    \n",
    "#     # Apply the weights to the loss (element-wise multiplication)\n",
    "#     weighted_loss = tf.multiply(loss, weights)  # Element-wise multiplication\n",
    "    \n",
    "#     # Return the mean loss across the batch and time steps\n",
    "#     return tf.reduce_mean(weighted_loss)\n",
    "\n",
    "# # Define the model\n",
    "# model = tf.keras.Sequential([\n",
    "#     tf.keras.layers.Input(shape=(None, 142)),  # Input layer for variable-length sequences with 142 features\n",
    "#     tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(256, return_sequences=True)),\n",
    "#     tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(256, return_sequences=True)),\n",
    "#     # tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(256, return_sequences=True)),\n",
    "#     # tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(256, return_sequences=True)),\n",
    "#     tf.keras.layers.Dense(64),  # Optional dense layer\n",
    "#     tf.keras.layers.Dense(35, activation='relu')  # Output layer, size 35 to match your output data dimension\n",
    "# ])\n",
    "\n",
    "# # Compile the model\n",
    "# optimizer = tf.keras.optimizers.Adam()\n",
    "# model.compile(optimizer=optimizer, loss=custom_loss)\n",
    "\n",
    "# # Assuming `dataset` is already prepared and `num_of_batches` is defined\n",
    "# # Training the model\n",
    "\n",
    "# dataset = dataset.repeat()\n",
    "# with tf.device('/gpu:0'):\n",
    "#     model.fit(dataset, epochs=2, steps_per_epoch=num_of_batches)\n",
    "# # After running, you can monitor GPU usage using `nvidia-smi` from the command line\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dotenv import load_dotenv\n",
    "# import os\n",
    "# #setting data path\n",
    "# load_dotenv()\n",
    "# project_path = os.getenv('PROJECT_PATH')\n",
    "# model.save(f'{project_path}/model_name.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_data = tf.ragged.constant([processed_audio_list[0]])  # Example input with sequence length 100 and 142 features\n",
    "\n",
    "# # Generate output using the loaded model\n",
    "# output = model.predict(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for h in output[0]:\n",
    "#     print(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from makeBeatmap import make_beatmap_from_output\n",
    "# make_beatmap_from_output(df.iloc[0],output[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
